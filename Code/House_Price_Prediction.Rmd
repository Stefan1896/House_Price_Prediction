---
title: "House Price Prediction with XGBoost"
author: "Stefan Werner"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = F, message = F, warning = F, comment = F)
options(scipen = 999999)
```

<p align="right">Please click here to visit my <a href="https://github.com/Stefan1896">github page</a>. <i class="fas fa-hand-peace"></i></span> </p>

# Problem definition


The goal of this challenge is to predict the sale price of residental homes in Ames, Iowa. The dataset comprises 79 explanatory variables. We will use a XGBoost Regression algorithm for prediction.

More information about data: <http://jse.amstat.org/v19n3/decock.pdf>

# Import data

Loading train and test data and get packages:

```{r}
if (!require(here)) install.packages('here')
if (!require(tidymodels)) install.packages('tidymodels')
if (!require(tidyverse)) install.packages('tidyverse')
if (!require(janitor)) install.packages('janitor')
if (!require(EnvStats)) install.packages('EnvStats')
if (!require(ggpubr)) install.packages('ggpubr')
if (!require(gridExtra)) install.packages('gridExtra')
if (!require(vip)) install.packages('vip')
if (!require(visdat)) install.packages('visdat')
if (!require(DataExplorer)) install.packages('DataExplorer')
if (!require(knitr)) install.packages('knitr')
                                             
training_data <- read_csv(here("Data", "train.csv")) %>%
                  clean_names()
testing_data <-  read_csv(here("Data", "test.csv")) %>%
                  clean_names()

combined <- bind_rows(training_data, testing_data)
train_rows <- nrow(training_data)
```
# Data Exploration and Preparation

Before modeling, lets check out some information about the data structure, get more information about the target variable and prepare relevant independent variables:

## Data Structure

Take a look at the data structure:

```{r}
vis_dat(combined)
```

Some character variables have a lots of missing data - we will drop columns with missing rate in the train set >40% 

```{r}
missing_plot <- DataExplorer::plot_missing(training_data, missing_only = T)
drop_cols <- missing_plot$data %>% filter(Band %in% c("Remove", "Bad")) %>% pull(feature)
```

## Target Variable

```{r}
# hist
g1 <- ggplot(training_data, aes(x=sale_price)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="indianred")+ 
  labs(x = "", y = "") +
  theme_classic() 

# boxplot
g2 <- ggplot(training_data, aes(y=sale_price)) + 
  geom_boxplot(aes(x=""), colour="black", fill="indianred", alpha=.2)+
  coord_flip()+ 
  labs(x = "", y = "") +
  theme_classic()

# qqplot
g3 <- ggplot(training_data, aes(sample = sale_price))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "") + 
  theme_classic()

grid.arrange(g1, arrangeGrob(g2, g3, nrow=2), nrow = 1)
```

Distribution is right-skewed. Lets see if Log transformation helps:

```{r}
# hist
g1 <- ggplot(training_data, aes(x=log(sale_price))) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="indianred")+ 
  labs(x = "", y = "") +
  theme_classic() 

# boxplot
g2 <- ggplot(training_data, aes(y=log(sale_price))) + 
  geom_boxplot(aes(x=""), colour="black", fill="indianred", alpha=.2)+
  coord_flip()+ 
  labs(x = "", y = "") +
  theme_classic()

# qqplot
g3 <- ggplot(training_data, aes(sample = log(sale_price)))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "") + 
  theme_classic()

grid.arrange(g1, arrangeGrob(g2, g3, nrow=2), nrow = 1)
```

Distribution looks way better after log transformation. 

## Exploratory Analysis

We will run a first regression without variables wich have missing data to get a first impression which variables are important in predicting Sale Price:

```{r}
regressionResult <- tidy(lm(log(sale_price) ~ ., data = training_data[ ,sapply(training_data, function(x) !any(is.na(x)))]))
kable(head(arrange(regressionResult, -statistic)))
```

The roof materials seems to have a big influence on Sale Price. Let us further look at this variable:

```{r}
kable(training_data %>% count(roof_matl))
```
Ups, the variable is highly unbalanced and has low variability, over 98% of houses have Standard (Composite) Shingle as Roof Material. Let us look again at the regression without roof material:

```{r}
regressionResult <- tidy(lm(log(sale_price) ~ ., data = subset(training_data[ ,sapply(training_data, function(x) !any(is.na(x)))], select = -c(roof_matl))))
kable(head(arrange(regressionResult, -statistic)))
```

We can see that Overall Quality has also a big influence on Sale Price. This variable is also more balanced, which is shown below. We will plot the 4 most important variables in their relation to share price:

```{r}
par(mfrow=c(2,2))

g1 <- ggplot(aes(as.factor(overall_qual), sale_price), data = training_data) +
  geom_bar(stat = "summary", fun = "mean", fill = "indianred") +
  stat_n_text(y.pos = 25000, size = 2) + 
  scale_fill_brewer(palette="Pastel1") +
  xlab("Quality") + 
  ylab("Sale Price") + 
  ggtitle("Price vs Overall Quality of Material") +
  theme_classic() 
#constant decrease in Sale Price with lower Roof Material

g2 <- ggplot(aes(x1st_flr_sf, sale_price), data = combined[!is.na(combined$sale_price),]) +
  geom_point(col = "indianred") +
  xlab("Second floor square feet") + 
  ylab("Sale Price") + 
  ggtitle("Price vs Second floor square feet") +
  theme_classic()


g3 <- ggplot(aes(reorder(ms_zoning,-sale_price), sale_price), data = training_data) +
  geom_bar(stat = "summary", fun = "mean", fill = "indianred") +
  stat_n_text(y.pos = 12500, size = 3) + 
  xlab("Zoning Classification") + 
  ylab("Sale Price") + 
  ggtitle("Price vs Zoning Classification") +
  #coord_cartesian(ylim = c(0, 400000)) + 
  theme_classic()

g4 <- ggplot(aes(as.factor(overall_cond), sale_price), data = training_data) +
  geom_bar(stat = "summary", fun = "mean", fill = "indianred") +
  stat_n_text(y.pos = 12500, size = 2) + 
  xlab("Quality") + 
  ylab("Sale Price") + 
  ggtitle("Price vs Overall Condition") +
  theme_classic()

ggarrange(g1, g2, g3, g4,
          ncol = 2, nrow = 2)
```

## Data Preprocessing

We will now fix typos, remove outliers and create further variables which could be useful in Sale Price Prediction:

```{r}
#fix typos
combined <- combined %>% mutate(roof_matl = recode(roof_matl,'Tar&Grv' = "Male")) %>% 
                          mutate(exterior1st = recode(exterior1st, 'Wd Sdng'="WdSdng")) %>%
                          mutate(exterior2nd = recode(exterior2nd,'Brk Cmn' = 'BrkComm', 'CmentBd' = 'CemntBd', 'Wd Sdng' = 'WdSdng', 'Wd Shng' = 'WdShing')) %>%
                          mutate(garage_yr_blt = replace(garage_yr_blt, garage_yr_blt > 2010, 'NA'))

#Outliers
ggplot(combined, aes(x=gr_liv_area, y=sale_price))+geom_point()+
  geom_text(data=combined[combined$gr_liv_area>4500,], mapping=aes(label=id), vjust=1.5, col = "red" ) +
  theme_classic()
combined <- combined %>% filter(!(gr_liv_area>4500 & !is.na(sale_price)))
```

The two marked points were removed, since they seem to be untypical from visual inspection and the model performance was better without them. Next, lets create new features:

```{r}
combined <- combined %>% mutate(has_bsmt = as.numeric(total_bsmt_sf!=0),
                          has_2ndFloor = as.numeric(x2nd_flr_sf!=0),
                          has_pool = as.numeric(pool_area!=0),
                          has_porch = as.numeric((open_porch_sf+enclosed_porch+x3ssn_porch+screen_porch)!=0),
                          has_remod = factor(year_remod_add != year_built),
                          has_fireplace = as.numeric(fireplaces >0),
                          is_new = as.numeric(yr_sold==year_built),
                          overall_sf = gr_liv_area + total_bsmt_sf,
                          overall_bath = full_bath + 0.5*half_bath + 0.5 *bsmt_half_bath + bsmt_full_bath,
                          house_age = yr_sold - year_remod_add,
                          yr_sold = factor(yr_sold),
                          mo_sold = factor(mo_sold),
                          tot_porch_sf = open_porch_sf + enclosed_porch +x3ssn_porch + screen_porch,
                          bsmt_bath = as.numeric(bsmt_half_bath + bsmt_full_bath !=0),
                          is_bsmt_unf = as.numeric(total_bsmt_sf == bsmt_unf_sf))

#update training and testing data that it includes all transformations
training_data <- combined %>% filter(!is.na(sale_price))
testing_Data <- combined %>% filter(is.na(sale_price))
```

# Model 

## Performance on cross-validation subset

Now, we will start the modeling part. As said before, a XGBoost regression model will be used. Further feature engineering will be done with the recipes package, since this provides comprehensive preprocessing possibilities in a nice and tidy code structue.

```{r}
set.seed(121)
vfold_data <- rsample::vfold_cv(data = training_data, v = 10)
```

Make specific recipe for XGBoost algorithm:

```{r}

xgb_recipe <- training_data %>%
  recipe(sale_price ~ .) %>%
  update_role(id, new_role = "id var") %>%
  step_rm(street, utilities) %>%
  step_rm(one_of(drop_cols)) %>%
  step_novel(all_predictors(), -all_numeric()) %>%
  step_impute_knn(all_predictors()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_nzv(everything()) %>%
  step_log(all_outcomes(), offset = 1) 
  

#Retrieve results of feature engineering (not necessary for model fitting).
#prepped_rec <- prep(xgb_recipe, verbose = TRUE, retain = TRUE)
#preproc_training_data <- recipes::bake(prepped_rec, new_data = NULL)
```

We will run a first quick xgboost model on cross-validation dataset to get an impression about performance and variable importance without tuning
```{r}
#specify model
xgb <-  parsnip::boost_tree(trees = 100) %>%
  set_engine("xgboost", objective = "reg:squarederror") %>%
  set_mode("regression")

xgb_wf <- workflows::workflow() %>%
  workflows::add_recipe(xgb_recipe) %>% 
  workflows::add_model(xgb)

# performance on cross-validation
xgb_fit <- xgb_wf %>% fit_resamples(vfold_data)
kable(collect_metrics(xgb_fit))
```

The cross-validation accuracy is `r round(collect_metrics(xgb_fit)$mean[1],2)` in terms of RMSE and `r round(collect_metrics(xgb_fit)$mean[2],2)` in terms of R-squared. After looking at variable importance, we will improve the accuracy with model tuning.

```{r}
#variable importance
xgb_wf %>% fit(training_data) %>%
  extract_fit_parsnip() %>% 
  vip()
```

## Tune Hyperparameters

Define model and parameter tuning:

```{r}
set.seed(1234)
xgb_tuning <- parsnip::boost_tree(
  trees = 1324, 
  tree_depth = tune(), 
  min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = 0.212, mtry = 80,              ## randomness
  learn_rate = 0.00893,                     ## step size
) %>% 
  set_engine("xgboost", objective = "reg:squarederror") %>%
  set_mode("regression")

xgb_grid <- grid_latin_hypercube(
  #trees(), 
  tree_depth(),
  min_n(),
  loss_reduction(),
  #sample_size = sample_prop(),
  #finalize(mtry(), training_data),
  #learn_rate(),
  size = 50
)

xgb_wf_tune <- workflows::workflow() %>%
  workflows::add_recipe(xgb_recipe) %>% 
  workflows::add_model(xgb_tuning)

#tune hyperparameters
xgb_res <- tune_grid(
  xgb_wf_tune,
  resamples = vfold_data,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

#look at tuning results
kable(head(xgb_res %>% collect_metrics %>% arrange(desc(.metric) ,-mean)))
```

The cross-validation accuracy after model tuning is `r round(collect_metrics(xgb_res)$mean[1],2)` in terms of RMSE and `r round(collect_metrics(xgb_res)$mean[2],2)` in terms of R-squared. Therefore, the accuracy was improved.

## Finalize model

Select best accuracy and finally fit model on whole training data to make test predictions.

```{r}
best_xgb <- xgb_res %>%
  select_best(metric = "rmse")

final_wf <- xgb_wf_tune %>% 
  finalize_workflow(best_xgb)

ind <- list(analysis = seq(nrow(training_data)), assessment = nrow(training_data) + seq(nrow(testing_data)))
splits <- make_splits(ind, combined)

final_fit <- final_wf %>% 
  last_fit(splits)

submission <- final_fit %>% collect_predictions() %>% transmute(ID = testing_data$id, SalePrice =  exp(.pred))

submission %>% write_csv(here("Data", "submission.csv"))
```

# Conclusion

With hyperparameter tuning, the XGBosst cross-validation model performance regarding R-Squared could be improved by `r round((collect_metrics(xgb_res)$mean[2]- collect_metrics(xgb_fit)$mean[2])*100,2)` percentage points. In total, `r round(collect_metrics(xgb_res)$mean[2]*100,1)`% of the variance in the training set can be explained from our prediction model. 



